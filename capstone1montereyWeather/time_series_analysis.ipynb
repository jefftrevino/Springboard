{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My notes on DataCamp's [Time Series Analysis in Python](https://campus.datacamp.com/courses/introduction-to-time-series-analysis-in-python/) course (1-5), notes on [Machine Learning Mastery](machinelearningmastery.com) tutorials (6-8), and my notes on Alina Zhang's [exponential smoothing, Holt, and Holt-Winters](https://medium.com/datadriveninvestor/how-to-build-exponential-smoothing-models-using-python-simple-exponential-smoothing-holt-and-da371189e1a1) Python models tutorial, with cross-reading from its source, [Forecasting: Principles and Practice](https://otexts.com/fpp2/) (9-11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "I. ARIMA Models\n",
    "1. [Understanding the Dickey-Fuller Test](#dftest)\n",
    "2. [Auto-Regressive (AR) Models](#ar)\n",
    "3. [Moving Average (MA) and ARMA Models](#ma)\n",
    "4. [Cointegration Models](#coint)\n",
    "5. [Case Study: Climate Change in NYC](#nyc)\n",
    "6. [ARIMA Models](#arima)\n",
    "7. [SARIMA Models](#sarima)\n",
    "8. [ARCH and GARCH Models](#garch)\n",
    "\n",
    "II. Exponential Models\n",
    "9. [Neither Trend nor Season? Simple (Single) Exponential Smoothing](#exp)\n",
    "10. [Trend but not Season? Trend Models (Double Exponential Smoothing)](#trend)\n",
    "11. [Trend and Season? Holt-Winters Seasonal Models (Triple Exponential Smoothing)](#season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dftest'></a>\n",
    "### 1. Understanding the Dickey-Fuller Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dickey-Fuller Test asks if a series acts like a random walk by asking if its differences act like white noise.\n",
    "\n",
    "In a random walk, the current value is equal to the last value plus some noise:\n",
    "\n",
    "\\begin{equation}\n",
    "y_t = ay_{t-1} + \\epsilon_t\n",
    "\\end{equation}\n",
    "\n",
    "If the value of $a$ is 1, then each term is equal to the last, except for the added error. The random walk is \"integrated of order 1,\" in that we must difference it once to get a stationary series. (A stationary series does not change its mean or variance over time; white noise is stationary.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the difference between two points in time is due completely to the added error, there should be no contributing slope component in the difference between two values:\n",
    "\n",
    "\\begin{equation}\n",
    "y_t - y_{t-1} = (\\phi - 1)y_{t-1} + \\epsilon_t\n",
    "\\end{equation}\n",
    "\n",
    "That is, mathematically, when we check the correlation between the difference series and the lagged series, the slope (correlation) coefficient of the lagged series ($\\phi - 1$) should be zero if the series behaves as a random walk (and its differences behave as white noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the Dickey-Fuller Test investigates: the Dickey-Fuller Test correlates a series' difference against its lag. If the correlation coefficient (slope) is 0 or very close to 0, the difference is just the noise and the current value series is a random walk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ar'></a>\n",
    "### 2. Auto-Regressive (AR) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an AR model of order 1 (AR(1)), today's value is a mean plus a fraction of yesterday's value plus noise:\n",
    "\n",
    "\\begin{equation}\n",
    "y_t = \\mu + \\phi y_{t-1} + \\epsilon_t\n",
    "\\end{equation}\n",
    "\n",
    "An AR(1) series can be stationary if $-1 \\lt \\phi \\lt 1$. A positive value for $\\phi$ means the series will behave a bit like a random walk and exhibit momentum, while a negative value means that values will alternate sign and exhibit mean reversion (and the series looks a bit like a waveform).\n",
    "\n",
    "Intuitively: if $\\phi$ is negative, the current value is the flipped sign of the last value, and the path of the series alternates its direction every other value. A positive $\\phi$ doesn't result in this alternation.\n",
    "\n",
    "The autocorrelation function of an AR(1) series decays exponentially: the lag(1) series will have autocorrelation $\\phi$, lag(2) $\\phi^2$, lag(3) $\\phi^3$, and lag(n) $\\phi^n$. A negative value just reverses the sign of the autocorrelation at each lag: the comparison alternates between same and opposite signs (see intuition above). Describing the current value with two values back is AR(2), three back AR(3), n back AR(n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating an AR process: general example\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "ar = np.array([1, -0.9]) # zero-lag coefficient of 1, phi value of 0.9 (! it's the opposite because signal processing)\n",
    "ma = np.array([1])\n",
    "AR_object = ArmaProcess(ar, ma)\n",
    "simulated_data = AR_object.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating an AR process: specific example with plot\n",
    "# import the module for simulating data\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "\n",
    "# Plot 1: AR parameter = +0.9\n",
    "plt.subplot(2,1,1)\n",
    "ar1 = np.array([1, -0.9])\n",
    "ma1 = np.array([1])\n",
    "AR_object1 = ArmaProcess(ar1, ma1)\n",
    "simulated_data_1 = AR_object1.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_1)\n",
    "\n",
    "# Plot 2: AR parameter = -0.9\n",
    "plt.subplot(2,1,2)\n",
    "ar2 = np.array([1, 0.9])\n",
    "ma2 = np.array([1])\n",
    "AR_object2 = ArmaProcess(ar2, ma2)\n",
    "simulated_data_2 = AR_object2.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating and Forecasting an AR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to estimate the some data set's $\\phi$, assuming an AR(1) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ARMA module from statsmodels\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "# Fit an AR(1) model to the first simulated data\n",
    "mod = ARMA(simulated_data_1, order=(1,0)) # fit to AR(1) model\n",
    "res = mod.fit()\n",
    "\n",
    "# Print out summary information on the fit\n",
    "print(res.summary())\n",
    "\n",
    "# Print out the estimate for the constant and for phi\n",
    "print(\"When the true phi=0.9, the estimate of phi (and the constant) are:\")\n",
    "print(res.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's how to forecast a particular date range with an AR(1) model using the `predict()` and `plot_predict()` methods. \"In-sample\" forecasting guesses one value after the existing data, while \"out-of-sample\" forecasts some values into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ARMA module from statsmodels\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "# Forecast the first AR(1) model\n",
    "mod = ARMA(simulated_data_1, order=(1,0))\n",
    "res = mod.fit()\n",
    "res.plot_predict(start=990, end=1010) # point 990 to point 1010 out of a 1,000 value series\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's tough to tell the difference between a random walk and a time series that mean reverts a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the Right Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you know the best order $p$ for an AR($p$) model? The partial autocorrelation function (PACF) and the information criteria help you choose the order of your model.\n",
    "\n",
    "The PACF measures the incremental benefit of adding another lag to the model function, by showing the magnitude of direct impact between past lag n and the current value (whereas ACF magnitudes show both direct and indirect impacts). `plot_pacf()` works like `plot_acf()`: one series argument and two kwargs, lag and alpha. The number of lags significantly (out of the blue signficance band you choose with alpha) different than zero tells you the order of the model.\n",
    "\n",
    "Information Criteria prevents overfitting by assessing a penalty for more model parameters. Two goodness-of-fit measures:\n",
    "\n",
    "1. AIC( Akaike Information Criterion) `result.aic`\n",
    "2. BIC (Bayesian Information Criterion) `result.bic`\n",
    "\n",
    "In practice, fit several models and then check out the BIC for each. Choose the model with the lowest value for the best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to plot the PACF for a series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules for simulating data and for plotting the PACF\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "# Simulate AR(1) with phi=+0.6\n",
    "ma = np.array([1])\n",
    "ar = np.array([1, -0.6])\n",
    "AR_object = ArmaProcess(ar, ma)\n",
    "simulated_data_1 = AR_object.generate_sample(nsample=5000)\n",
    "\n",
    "# Plot PACF for AR(1)\n",
    "plot_pacf(simulated_data_1, lags=20)\n",
    "plt.show()\n",
    "\n",
    "# Simulate AR(2) with phi1=+0.6, phi2=+0.3\n",
    "ma = np.array([1])\n",
    "ar = np.array([1, -0.6, -0.3])\n",
    "AR_object = ArmaProcess(ar, ma)\n",
    "simulated_data_2 = AR_object.generate_sample(nsample=5000)\n",
    "\n",
    "# Plot PACF for AR(2)\n",
    "plot_pacf(simulated_data_2, lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to plot the BIC as a function of model order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for estimating an ARMA model\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "# Fit the data to an AR(p) for p = 0,...,6 , and save the BIC\n",
    "BIC = np.zeros(7)\n",
    "for p in range(7):\n",
    "    mod = ARMA(simulated_data_2, order=(p,0))\n",
    "    res = mod.fit()\n",
    "# Save BIC for AR(p)    \n",
    "    BIC[p] = res.bic\n",
    "    \n",
    "# Plot the BIC as a function of p\n",
    "plt.plot(range(1,7), BIC[1:7], marker='o')\n",
    "plt.xlabel('Order of AR Model')\n",
    "plt.ylabel('Bayesian Information Criterion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ma'></a>\n",
    "### 3. Moving Average (MA) and ARMA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an MA(1) model (moving average model of order 1), today's value is a mean pluse noise plus a fraction theta of yesterday's noise:\n",
    "\n",
    "\\begin{equation}\n",
    "y_t = \\mu + \\epsilon + \\theta \\epsilon_{t-1}\n",
    "\\end{equation}\n",
    "\n",
    "This is of order 1, because there's one lagged value in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\theta$ is 0, the process is white noise shifted by a mean. MA models are stationary for all values of theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If \\theta is negative, then a positive value last period means the next value is likely to be negative; a positive value two values might not have any effect two values later.\n",
    "\n",
    "lag(1) autocorrelation for MA models is not $\\theta$ but $\\theta/(1+\\theta)^2$. When $\\theta$ is negative, lag(1) autocorrelation is negative. When $\\theta$ is positive, lag(1) autocorrelation is positive. An MA(n) has no autocorrelation beyond lag(n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to simulate an MA process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the module for simulating data\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "\n",
    "# Plot 1: MA parameter = -0.9\n",
    "plt.subplot(2,1,1)\n",
    "ar1 = np.array([1])\n",
    "ma1 = np.array([1, -.9])\n",
    "MA_object1 = ArmaProcess(ar1, ma1)\n",
    "simulated_data_1 = MA_object1.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_1)\n",
    "\n",
    "# Plot 2: MA parameter = +0.9\n",
    "plt.subplot(2,1,2)\n",
    "ar2 = np.array([1])\n",
    "ma2 = np.array([1, 0.9])\n",
    "MA_object2 = ArmaProcess(ar2, ma2)\n",
    "simulated_data_2 = MA_object2.generate_sample(nsample=1000)\n",
    "plt.plot(simulated_data_2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to compute the autocorrelation for an MA series (as before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the plot_acf module from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Plot 1: MA parameter = -0.9\n",
    "plot_acf(simulated_data_1, lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating and Forecasting an MA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just use statsmodel's `ARMA` class as with an AR model, but pass in `order=(0,1)` in place of `order=(1,0)` for an MA model. You can forecast a single value after data, and then one more value after that, after which all values will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARMA Models\n",
    "\n",
    "In an AR model, the current value has to do with n previous values. In an MA model, the current value has to do with n previous noise components. In an ARMA model, the current value has to do with both past values and past noise components.\n",
    "\n",
    "In an ARMA(1, 1) model, the current value is equal to a mean plus some fraction $\\phi$ of the last value plus some noise component plus some fraction $\\theta$ of the last noise component.\n",
    "\n",
    "\\begin{equation}\n",
    "y_t = \\mu + \\phi y_{t-1} + \\epsilon + \\theta \\epsilon_{t-1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to convert an ARMA  model of order n into an AR model of order inifinity or an MA model of order infinity by substitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Neat Trick:* It' useful to find the indices of missing data by subtracting the index from a complete range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything\n",
    "set_everything = set(range(391))\n",
    "\n",
    "# The intraday index as a set\n",
    "set_intraday = set(intraday.index)\n",
    "\n",
    "# Calculate the difference\n",
    "set_missing = set_everything - set_intraday\n",
    "\n",
    "# Print the difference\n",
    "print(\"Missing rows: \", set_missing)\n",
    "\n",
    "# reindex with forward fill to replace missing values\n",
    "intraday = intraday.reindex(range(391), method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming a numeric index into a datetime index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a `date_range` method. Make a date range and set it as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From previous step\n",
    "intraday = intraday.reindex(range(391), method='ffill')\n",
    "\n",
    "# Change the index to the intraday times\n",
    "intraday.index = pd.date_range(start='2017-09-01 9:30', end='2017-09-01 16:00', freq='1min')\n",
    "\n",
    "# Plot the intraday time series\n",
    "intraday.plot(grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying an MA Model\n",
    "\n",
    "If you see no autocorrelations at lags higher than n, you should fit an MA(n) model to the data. Compute the difference series using `pct_change()` and then fit an MA model to the difference series, remembering to use `drop_na()` to get rid of the first row's `NaN` (because differences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plot_acf and ARMA modules from statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "# Compute returns from prices and drop the NaN\n",
    "returns = intraday.pct_change()\n",
    "returns = returns.dropna()\n",
    "\n",
    "# Plot ACF of returns with lags up to 60 minutes\n",
    "plot_acf(returns, lags=60)\n",
    "plt.show()\n",
    "\n",
    "# Fit the data to an MA(1) model\n",
    "mod = ARMA(returns, order=(0,1))\n",
    "res = mod.fit()\n",
    "print(res.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulating an AR(1) model as an MR($\\infty$) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AR(1) model is the same as an MR($\\infty$) model:\n",
    "\n",
    "\\begin{equation}\n",
    "y_t = \\mu + \\phi y_{t-1} + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "y_t = \\mu + \\sum_{i=1}^{\\infty} \\phi^i \\epsilon_{t-i}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simulation of an AR(1) model with $\\phi$=0.8 as an MR($\\infty$) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules for simulating data and plotting the ACF\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Build a list MA parameters\n",
    "ma = [0.8**i for i in range(30)]\n",
    "\n",
    "# Simulate the MA(30) model\n",
    "ar = np.array([1])\n",
    "AR_object = ArmaProcess(ar, ma)\n",
    "simulated_data = AR_object.generate_sample(nsample=5000)\n",
    "\n",
    "# Plot the ACF\n",
    "plot_acf(simulated_data, lags=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='coint'></a>\n",
    "### 4. Cointegration Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if two series are each independently a random walk, it's still possible that their linear combination isn't a random walk and is therefore forecastable. For example: the distance between a dog and its walking owner is mean reverting, although the owner's position and the dog's positions might be random walks, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Two Series for Cointegration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have two series $P_t$ and $Q_t$ and want to check for their cointegration,\n",
    "1. Regress $P_t$ on $Q_t$ to get a slope (correlation coefficient) $c$.\n",
    "2. Test the linear combination of the two series, $P_t - cQ_t$, for random walkness using the Augmented Dickey Fuller test.\n",
    "\n",
    "In Python this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statstools import coint\n",
    "coint(P, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be helpful to plot the two series separately, as well as their difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prices separately\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(7.25*HO, label='Heating Oil')\n",
    "plt.plot(NG, label='Natural Gas')\n",
    "plt.legend(loc='best', fontsize='small')\n",
    "\n",
    "# Plot the spread\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(7.25*HO-NG, label='Spread')\n",
    "plt.legend(loc='best', fontsize='small')\n",
    "plt.axhline(y=0, linestyle='--', color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the ADF test to each series separately, and then to their difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the adfuller module from statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Compute the ADF for HO and NG\n",
    "result_HO = adfuller(HO['Close'])\n",
    "print(\"The p-value for the ADF test on HO is \", result_HO[1])\n",
    "result_NG = adfuller(NG['Close'])\n",
    "print(\"The p-value for the ADF test on NG is \", result_NG[1])\n",
    "\n",
    "# Compute the ADF of the spread\n",
    "result_spread = adfuller(7.25 * HO['Close'] - NG['Close'])\n",
    "print(\"The p-value for the ADF test on the spread is \", result_spread[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's an example of the two-step process from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the statsmodels module for regression and the adfuller function\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Regress BTC on ETH\n",
    "ETH = sm.add_constant(ETH) # add column of 1s to ETH to get a y-intercept\n",
    "result = sm.OLS(BTC,ETH).fit()\n",
    "\n",
    "# Compute ADF\n",
    "b = result.params[1]\n",
    "adf_stats = adfuller(BTC['Price'] - b*ETH['Price'])\n",
    "print(\"The p-value for the ADF test is \", adf_stats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nyc'></a>\n",
    "### 5. Case Study: Climate Change in NYC and ARIMA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average annual temperature data act like a random walk with drift (ADF test result is 0.58):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the adfuller function from the statsmodels module\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Convert the index to a datetime object\n",
    "temp_NY.index = pd.to_datetime(temp_NY.index, format='%Y')\n",
    "\n",
    "# Plot average temperatures\n",
    "temp_NY.plot()\n",
    "plt.show()\n",
    "\n",
    "# Compute and print ADF p-value\n",
    "result = adfuller(temp_NY['TAVG'])\n",
    "print(\"The p-value for the ADF test is \", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a random walk with drift stationary, take its first difference. Then we'll compute the ACF and the PACF to get guidance on which model to use. We see lag(1) as the only significant value, and it's negative. This means we should model this as an MA(1) model with a negative $\\theta$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules for plotting the sample ACF and PACF\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Take first difference of the temperature Series\n",
    "chg_temp = temp_NY.diff()\n",
    "chg_temp = chg_temp.dropna()\n",
    "\n",
    "# Plot the ACF and PACF on the same page\n",
    "fig, axes = plt.subplots(2,1)\n",
    "\n",
    "# Plot the ACF\n",
    "plot_acf(chg_temp, lags=20, ax=axes[0])\n",
    "\n",
    "# Plot the PACF\n",
    "plot_pacf(chg_temp, lags=20, ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the data to AR(1), AR(2), and ARMA(1,1) models to see which one has the lowest Akaike Information Criterion (AIC) value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for estimating an ARMA model\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "# Fit the data to an AR(1) model and print AIC:\n",
    "mod_ar1 = ARMA(chg_temp, order=(1, 0))\n",
    "res_ar1 = mod_ar1.fit()\n",
    "print(\"The AIC for an AR(1) is: \", res_ar1.aic)\n",
    "\n",
    "# Fit the data to an AR(2) model and print AIC:\n",
    "mod_ar2 = ARMA(chg_temp, order=(2, 0))\n",
    "res_ar2 = mod_ar2.fit()\n",
    "print(\"The AIC for an AR(2) is: \", res_ar2.aic)\n",
    "\n",
    "# Fit the data to an ARMA(1,1) model and print AIC:\n",
    "mod_arma11 = ARMA(chg_temp, order=(1,1))\n",
    "res_arma11 = mod_arma11.fit()\n",
    "print(\"The AIC for an ARMA(1,1) is: \", res_arma11.aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ARMA(1,1) model has the lowest AIC value of the three, which means it fits the data the best of the three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA Models\n",
    "\n",
    "Using an ARIMA model on the data is identical to using an ARMA model on the data's differences and then taking cumulative sums of temperature change to forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot predictions using an ARMA(1,1) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ARIMA module from statsmodels\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "# Forecast temperatures using an ARIMA(1,1,1) model\n",
    "mod = ARIMA(temp_NY, order=(1,1,1)) #pdq d is 1 because we differenced once\n",
    "res = mod.fit()\n",
    "\n",
    "# Plot the original series and the forecasted series\n",
    "res.plot_predict(start='1872-01-01', end='2046-01-01')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"According to the model, the temperature in NYC is expected to be about 0.6 degrees higher in 30 years (almost entirely due to the trend), but the 95% confidence interval around that is over 5 degrees.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More complex topics to explore:\n",
    "- GARCH Models\n",
    "- Nonlinear Models\n",
    "- Multivariate Time Series Models\n",
    "- Regime Switching Models\n",
    "- State Space Models and Kalman Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arima'></a>\n",
    "### 6. ARIMA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA adds integration to ARMA. Auto-Regressive Integrated Moving Average model. To recap what that means from above:\n",
    "\n",
    "- AR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n",
    "\n",
    "- I: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n",
    "\n",
    "- MA: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n",
    "\n",
    "The ARIMA model takes these three dimensions as three parameters:\n",
    "\n",
    "- p: The number of lag observations included in the model, also called the lag order.\n",
    "- d: The number of times that the raw observations are differenced, also called the degree of differencing.\n",
    "- q: The size of the moving average window, also called the order of moving average.\n",
    "\n",
    "The $p$ and $q$ can be guessed based on the autocorrelation plots. But [how do you know how many times to difference](https://people.duke.edu/~rnau/411arim2.htm)?\n",
    "\n",
    "If q is 0, you get an ARMA model. If p and q are zero, you get an MA model. If d and q are 0, you get an AR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we want to make an ARIMA model of some random-walk looking monthly shampoo sales data,\n",
    "\n",
    "- p=5, because an autocorrelation plot of the values shows significant values for lags through 5.\n",
    "- d=1, because you difference a random walk once to make it stationary\n",
    "- q=0, because ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = read_csv('shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "# fit model\n",
    "model = ARIMA(series, order=(5,1,0))\n",
    "model_fit = model.fit(disp=0)\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot the residuals to see if there's trend information the models' missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residual errors\n",
    "residuals = DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "pyplot.show()\n",
    "residuals.plot(kind='kde')\n",
    "pyplot.show()\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows a bias in the prediction: the mean of the residuals is -5, not 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sarima'></a>\n",
    "### 7. SARIMA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA can handle data with a trend, but it doesn't deal with seasonal components. ARIMA expects data with a trend only. If data have a seasonal component, seasonal differencing needs to be performed before analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARIMA adds a separate ARIMA model for the seasonal component. That means four new parameters:\n",
    "\n",
    "- pdq as above for the seasonal component's ARIMA model\n",
    "- a fourth parameter specifying the period of the seasonal component\n",
    "\n",
    "Pass in the trend and seasonal parameters as two 3- and 4-value tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.sarimax import SARIMAX\n",
    "\n",
    "# define model configuration\n",
    "my_order = (1, 1, 1)\n",
    "my_seasonal_order = (1, 1, 1, 12)\n",
    "# define model\n",
    "model = SARIMAX(data, order=my_order, seasonal_order=my_seasonal_order, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting and prediction work as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model_fit = model.fit()\n",
    "# one step forecast\n",
    "yhat = model_fit.forecast()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='garch'></a>\n",
    "### 8. ARCH and GARCH Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the variance is shifting over time in some systematic way (data exhibit heteroskedasticity), AR, MA, ARMA, ARIMA, and SARIMA models will not model the data very well. Enter ARCH (Engle, 1982):\n",
    "\n",
    "__A__ Auto-\n",
    "\n",
    "__R__ Regressive\n",
    "\n",
    "__C__ Conditional\n",
    "\n",
    "__H__ Heteroskedasticity\n",
    "\n",
    "...to model the variance of the variance over time.\n",
    "\n",
    "It models variance as a function of residuals from a zero mean process. $q$ is the number of lag-squared residual errors to include in the model. (It's called $p$ sometimes, but you need to call it $q$ for the next bit on GARCH to make any sense.)\n",
    "\n",
    "ARCH expects a stationary series that doesn't have a trend or seasonal component. So you need to fit one of the above models first, and then run the residuals through ARCH or GARCH."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GARCH is ARCH with a moving average. A GARCH model has two parameters to model the value of current variance $\\sigma^2_t$:\n",
    "\n",
    "- $p$ past variances in the model\n",
    "\n",
    "- $q$ past errors in the model\n",
    "\n",
    "For example, a GARCH model of order 1 (GARCH(1,1)) models current variance as a mean plus a fraction of the square of the lag noise plus a fraction of the square of the lag variance:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_t = \\mu + \\phi \\epsilon_{t-1}^2 + \\theta \\sigma_{t-1}^2\n",
    "\\end{equation}\n",
    "\n",
    "Just as ARMA subsumes AR and MA models, GARCH subsumes ARCH. A GARCH(0,q) model is an ARCH(q) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring ARCH and GARCH Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to approach the variance of the series like you approached the difference above: have a look at the ACF and PACF of the variance.\n",
    "\n",
    "1. Subtract the mean from each value in the series.\n",
    "2. Square the result.\n",
    "3. Plot the ACF and PACF.\n",
    "4. Interpret these plots like you did above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARCH and GARCH in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [arch](https://github.com/bashtage/arch) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "am = arch_model(data)\n",
    "res = am.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can model a linear increase in variance using a draw from a Gaussian distribution with an expanding standard deviation (increasingly spastic variation about a mean of 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "data = [gauss(0, i*0.01) for i in range(1,100+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The squared data show significant autocorrelation out to about 15 lags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_data = [x**2 for x in data]\n",
    "# create acf plot\n",
    "plot_acf(squared_data)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15 becomes the value passed into the ARCH model on configuration. We fit the model and then use it to forecast. This code defines the model and forecasts the variance of the last ten timesteps in the dataset. It plots the actual variance against the forecast variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = arch_model(data, mean='Zero', vol='ARCH', p=15)\n",
    "# fit model\n",
    "model_fit = model.fit()\n",
    "# forecast the test set\n",
    "yhat = model_fit.forecast(horizon=10)\n",
    "# plot the actual variance\n",
    "var = [i*0.01 for i in range(0,100)]\n",
    "pyplot.plot(var[-n_test:])\n",
    "# plot forecast variance\n",
    "pyplot.plot(yhat.variance.values[-1, :])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a GARCH model works similarly, but you configure two parameters now and specify `vol='GARCH'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = arch_model(train, mean='Zero', vol='GARCH', p=15, q=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exp'></a>\n",
    "### 9. Neither Trend nor Season? Simple (Single) Exponential Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How important should a past observation be in predicting the future? Exponential smoothing makes past observations exponentially less important as they recede into the past, relative to the moment of the last data value. This is normally used for short-term forecasts, as the technique's long-term forecasts aren't so reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for exponential smoothing sections\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't see a clear seasonal pattern and also don't see a clear trend, just calculate a weighted average of past observations that weights older observations less:\n",
    "\n",
    "\\begin{equation}\n",
    "y_t = \\sum_{i=1}^{n}\\alpha(1-\\alpha)^i y_{t-i}\n",
    "\\end{equation}\n",
    "\n",
    "$\\alpha$ is a *smoothing parameter* that controls the slope of the weight decrease. As i increases, the series of values regresses into the past, and the exponent increases by 1 with each older value, multiplying each successively older value by a smaller and smaller value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a = 0.1</th>\n",
       "      <th>a = 0.2</th>\n",
       "      <th>a = 0.3</th>\n",
       "      <th>a = 0.4</th>\n",
       "      <th>a = 0.5</th>\n",
       "      <th>a = 0.6</th>\n",
       "      <th>a = 0.7</th>\n",
       "      <th>a = 0.8</th>\n",
       "      <th>a = 0.9</th>\n",
       "      <th>a = 1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothing exponent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9000</td>\n",
       "      <td>1.6000</td>\n",
       "      <td>2.1000</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>2.5000</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>2.1000</td>\n",
       "      <td>1.6000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8100</td>\n",
       "      <td>1.2800</td>\n",
       "      <td>1.4700</td>\n",
       "      <td>1.4400</td>\n",
       "      <td>1.2500</td>\n",
       "      <td>0.9600</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7290</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>1.0290</td>\n",
       "      <td>0.8640</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>0.1890</td>\n",
       "      <td>0.0640</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.8192</td>\n",
       "      <td>0.7203</td>\n",
       "      <td>0.5184</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.1536</td>\n",
       "      <td>0.0567</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    a = 0.1  a = 0.2  a = 0.3  a = 0.4  a = 0.5  a = 0.6  \\\n",
       "smoothing exponent                                                         \n",
       "0                    1.0000   2.0000   3.0000   4.0000   5.0000   6.0000   \n",
       "1                    0.9000   1.6000   2.1000   2.4000   2.5000   2.4000   \n",
       "2                    0.8100   1.2800   1.4700   1.4400   1.2500   0.9600   \n",
       "3                    0.7290   1.0240   1.0290   0.8640   0.6250   0.3840   \n",
       "4                    0.6561   0.8192   0.7203   0.5184   0.3125   0.1536   \n",
       "\n",
       "                    a = 0.7  a = 0.8  a = 0.9  a = 1.0  \n",
       "smoothing exponent                                      \n",
       "0                    7.0000   8.0000   9.0000     10.0  \n",
       "1                    2.1000   1.6000   0.9000      0.0  \n",
       "2                    0.6300   0.3200   0.0900      0.0  \n",
       "3                    0.1890   0.0640   0.0090      0.0  \n",
       "4                    0.0567   0.0128   0.0009      0.0  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "alphas = [x/10 for x in range(1, 11)] # get alphas of .2, .4, .6, .8\n",
    "value_df = pd.DataFrame([])\n",
    "\n",
    "for alpha in alphas:\n",
    "     value_df[str(alpha)] = [alpha*(1-alpha)**i *10 for i in range(5)] # smooth move\n",
    "value_df.columns = ['a = ' + str(x) for x in value_df.columns]\n",
    "value_df.index = value_df.index.rename('smoothing exponent')\n",
    "value_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A table inspired by [Forecasting: Principles and Practice](https://otexts.com/fpp2/ses.html): A higher $\\alpha$ favors recent values and approaches forecast based entirely on the last observation. A lower $\\alpha$ approaches an average of all past values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "for i in value_df.index:\n",
    "    plt.plot(value_df.loc[i,:], marker='.', linestyle='none')\n",
    "plt.xlabel('smoothing parameter')\n",
    "plt.ylabel('smoothed values for a list of five 10s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above: an increasing smoothing parameter boosts the most recent value and damps all other values more and more until $\\alpha=1$ bases the forecast on only the last value, with all other values equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to fit a simple exponential smoothing model to data in statsmodels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"data\" is a Pandas dataframe\n",
    "result = SimpleExpSmoothing(data).fit()\n",
    "forecast = result.forecast(12).rename(r'$\\alpha=%s$'%result.model.params['smoothing_level'])\n",
    "# plot\n",
    "result.plot(marker='o', color='green', legend=True)\n",
    "result.fittedvalues.plot(marker='o', color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trend'></a>\n",
    "### 10. Trend but not Season? Trend Models (Double Exponential Smoothing)\n",
    "\n",
    "#### 10a. Holt Linear Trend Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for exponential smoothing sections\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holt (1957) models a forecast as linear function of $h$, how many steps ahead the forecast is. The function's y-intercept is the level and the function's slope the trend value $b_t$. Whereas simple exponential smoothing provides a flat forecast, the Holt model creates a forecast with a trend: The $h$-step-ahead forecast is equal to the last estimated level plus $h$ times the last estimated trend value.\n",
    "\n",
    "\\begin{equation}\n",
    "y_{t+h|t} = l_t + hb_t\n",
    "\\end{equation}\n",
    "\n",
    "We have two smoothing parameters now, $\\alpha$ for the level and $\\beta$ for the trend, as used in the level and trend equations \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*level* \n",
    "\n",
    "The equation for the current level weights by $\\alpha$ the current value and the sum of the last value's forecast parameters (level and trend):\n",
    "\n",
    "\\begin{equation}\n",
    "l_t = \\alpha y_t + (1-\\alpha)(l_{t-1} + b_{t-1})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*trend*\n",
    "\n",
    "The equation for the current trend weights by $\\beta$ (1) the change in level between the last value and the current value and (2) the last value's trend:\n",
    "\n",
    "\\begin{equation}\n",
    "b_t = \\beta(l_t - l_{t-1}) + (1-\\beta)b_{t-1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of $\\beta$ as change in slope over time: a high $\\beta$ means that the past slope is almost entirely determining the present slope. A low $\\beta$ changes slope a lot, because the last trend estimate influences current slope.\n",
    "\n",
    "Likewise, the $\\alpha$ value fades between the truth and the model's prediction of the truth: a low $\\alpha$ value bases the current level almost entirely on the last value's forecast parameters, while a high $\\alpha$ value bases the current level almost entirely on the current value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit1 = Holt(data).fit(smoothing_level=0.8, smoothing_slope=0.2, optimized=False) # linear model\n",
    "fcast1 = fit1.forecast(12).rename(\"Holt's linear trend\")\n",
    "\n",
    "fit2 = Holt(data, exponential=True).fit(smoothing_level=0.8, smoothing_slope=0.2, optimized=False) # exponential model\n",
    "fcast2 = fit2.forecast(12).rename(\"Exponential trend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10b. Damped Holt Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holt models tend to overforecast the further out they get. Gardner and McKenzie (1985) introduced formulas that dampen the trend to a flat line over time by adding a damping coefficient $\\phi$ that causes the slope's growth to gradually approach zero by exponentially dampening more for each day into the future (as days into the future $h$ increases): \n",
    "\n",
    "\\begin{equation}\n",
    "y_{t+h|t} = l_t + b_t \\sum_{i=1}^{h} \\phi^h\n",
    "\\end{equation}\n",
    "\n",
    "Moreover, the damping impacts the currenet value's level and trend by scaling the last value's measured slope (trend):\n",
    "\n",
    "\\begin{equation}\n",
    "l_t = \\alpha y_t + (1-\\alpha)(l_{t-1} + \\phi b_{t-1})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "b_t = \\beta(l_t - l_{t-1}) + (1-\\beta)\\phi b_{t-1}\n",
    "\\end{equation}\n",
    "\n",
    "In theory, $0 \\le \\phi \\le 1$, but in practice $.8 \\le \\phi \\le .98$, because the damping has too strong an effect for small values and behaves like a non-damped model too close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As days into the future $h$ increases, the forecast value converges to a value given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\lim_{h\\to\\infty} y_{t+h|h} = \\frac{l_t + \\phi b_t}{(1-\\phi)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit a dampened linear Holt model in Python, specify `smoothing_level` ($\\alpha$) and `smoothing_slope` ($\\beta$) and let the model optimize $\\phi$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit3 = Holt(data, damped=True).fit(smoothing_level=0.8, smoothing_slope=0.2)\n",
    "fcast3 = fit3.forecast(12).rename(\"Additive damped trend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='season'></a> \n",
    "### 11. Trend and Season? Holt-Winters Seasonal Models (Triple Exponential Smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holt and his student Winters (1960) added a third smoothing equation to the forecast beyond level and trend: the seasonal component $s_t$ smoothed by a third smoothing parameter $\\gamma$. The seasonal smoothing equation introduces another variable $m$ to capture the period of the seasonal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Sorts of Seasons\n",
    "\n",
    "If seasonal component remains constant, use an additive model. If the seasonal component changes with respect to the level value, use a multiplicative model of the seasonal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11a. Holt-Winters' Additive Method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a seasonal component to the forecast equation and to the level equation.\n",
    "\n",
    "*the forecast*\n",
    "\n",
    "We add a new seasonal term, $s_{t+h-m(k+1)}$, to factor in the last year's seasonal estimate on the corresponding day in the last seasonal cycle. To be sure we take the value from the last seasonal cycle, the new value $k$ in the seasonal component of the forecast is the integer part of $(h-1)/m$. Algorithmically,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-18"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 30 # 30-day seasonal period\n",
    "h = 72 # we're looking 72 days into the future; the 12th day of the period\n",
    "k = (h-1)//m # divide by how many periods into the future we're going\n",
    "h - m*(k + 1)# look up the seasonal estimate from the corresponding day in the last period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h - m(k+1)$ is -18 here to indicate that we would take the seasonal estimate from day 12 of the data's final period (18 days into the past). So the whole forecast becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "y_{t+h|t} = l_t + hb_t + s_{t+h-m(k+1)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*the level component*\n",
    "\n",
    "The level parameter $\\alpha$ weights an average between the last period's estimated parameter and the last value's seasonless forecast (the sum of its level and trend).\n",
    "\n",
    "\\begin{equation}\n",
    "l_t = \\alpha (y_t - s_{t-m}) + (1-\\alpha)(l_{t-1} + b_{t-1})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*the trend component*\n",
    "\n",
    "The trend stays the same as the linear model...because it's linear. $\\beta$ weights an average between the difference in the level and the last value's trend.\n",
    "\n",
    "\\begin{equation}\n",
    "b_t = \\beta(l_t - l_{t-1}) + (1-\\beta)b_{t-1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*the season component*\n",
    "\n",
    "The season component's \\gamma parameter weights an avereage between the current value's seasonal component (the current value minus the last value's level and trend predictions) and the last period's corresponding value's seasonal component:\n",
    "\n",
    "\\begin{equation}\n",
    "s_t = \\gamma (y_t - l_{t-1} - b_{t-1}) + (1-\\gamma)s_{t-m}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11b. Holt-Winters' Multiplicative Method\n",
    "\n",
    "The idea of the components is more or less the same, but you multiply them instead of add them to move into the future, and you divide them instead of difference them to get back to past values:\n",
    "\n",
    "\\begin{equation}\n",
    "y_{t+h|t} = (l_t + hb_t)s_{t+h-m(k+1)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "l_t = \\alpha \\frac{y_t}{s_{t-m}} + (1-\\alpha)(l_{t-1} + b_{t-1})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "b_t = \\beta(l_t - l_{t-1}) + (1-\\beta)b_{t-1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "s_t = \\gamma \\frac{y_t}{l_{t-1} + b_{t-1}} + (1-\\gamma)s_{t-m}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11c. Damped Holt-Winters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds damping to the change in trend as above. Damped trend with multiplicative seasonality often performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python,\n",
    "\n",
    "1. additive trend, additive season\n",
    "\n",
    "1. additive trend, multiplicative season\n",
    "\n",
    "1. additive trend, additive season with damping\n",
    "\n",
    "1. additive trend, multiplicative season with damping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit1 = ExponentialSmoothing(saledata, seasonal_periods=4, trend='add', seasonal='add').fit(use_boxcox=True)\n",
    "\n",
    "fit2 = ExponentialSmoothing(saledata, seasonal_periods=4, trend='add', seasonal='mul').fit(use_boxcox=True)\n",
    "\n",
    "fit3 = ExponentialSmoothing(saledata, seasonal_periods=4, trend='add', seasonal='add', damped=True).fit(use_boxcox=True)\n",
    "\n",
    "fit4 = ExponentialSmoothing(saledata, seasonal_periods=4, trend='add', seasonal='mul', damped=True).fit(use_boxcox=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
